{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Analysis Of Northern Yucatan Coast for Kiteboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "This is the list of libraries that are needed. Most of these libraries are available in Anaconda and Databricks with exception of calmap for calendar plots and astral for sunrise and sunset times. They have to be installed to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Dataframe:\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "# Daylight UDF:\n",
    "import astral\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Plot related\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import calmap\n",
    "\n",
    "# Math\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIVE_MINUTES_IN_SECS = (5*60) # Seconds\n",
    "TWO_HOURS_IN_5MINS = (2*60)//5 # This iw hos many 5 min periods there is in 2 hours in \n",
    "MIN_SAMPLE_COUNT = 100\n",
    "DUMP_CSV=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, f):\n",
    "    return f(self)\n",
    "\n",
    "DataFrame.transform = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "The weather station (PWS) used for this analysis is CINVESTAV Telchac located in the Northern Yucantan coast. Coordinates of the station are latitude 21.341108 and longitude -89.305756. This is a single PWS project so spark will be configured in the timeszone of the project and there will be a location object for the PWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwsID = 'IYUCATNT2'\n",
    "pwsTz = 'America/Merida'\n",
    "sensorLoc = astral.Location((\"Telchac\", \"Mexico\", 21.341108, -89.305756, pwsTz, 0))\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", pwsTz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information was gathered from Weather Underground web site that provides a free API call to gather the information of a particular day. The weather station ID for this location is IYUCATNT2. The response comes almost in a CSV format except for the insertion of br html tags.\n",
    "\n",
    "For instance the following URL will provide the data for January 1st, 2016:\n",
    "\n",
    "https://www.wunderground.com/weatherstation/WXDailyHistory.asp?ID=IYUCATNT2&month=1&day=1&year=2016&format=1\n",
    "\n",
    "A crawler script (with some wait between request) was created to dump the sensor information and do the simple task of converting it to a CSV (removing the inserted tags). The data is organized in subdirectories for the station ID, year and month for easy management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks location:\n",
    "#fileStorePath = \"/FileStore/tables/test1\"\n",
    "    \n",
    "fileStorePath = \"./\"\n",
    "\n",
    "unformattedWindDF = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"{}/dailyHistory/{}/*/*/*.csv\".format(fileStorePath,pwsID))\n",
    "\n",
    "#unformattedWindDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The sensor has historical data only from the middle of 2008 so the range for this analysis is from 2009 to 2018.\n",
    "\n",
    "The table that Weather Underground returns has a lot of columns that represent measurements from the PWS like pressure, temperature, dew point, etc. but for this analysis we only focused on the following:\n",
    "\n",
    "__Time__ : This is the timestamp at which the sample is taken in the local timezone of the PWS. There is another column that has the time in UTC but not used in this analysis. This column is kept as a string for comparison.\n",
    "\n",
    "__WindDirectionDegrees__ : This is the wind direction in degrees. The cardinal directions map as follow: North is 0, East is 90, South is 180 and West is 270. This is casted to integer.\n",
    "\n",
    "__WindSpeedMPH__ : Wind speed measured in miles per hour (mph). There is also a gust wind speed column but not used in this analysis. This is casted to a double.\n",
    "\n",
    "The following columns are derived from the previous:\n",
    "\n",
    "__TS__ : Conversion of the _Time_ string to a timestamp. Casting from string to timestamp was the best method posible found to handle timestamps in a the PWS timezone from the local timezone in spark.\n",
    "\n",
    "__Date__ : Extraction of the date portion of the timestamp.\n",
    "\n",
    "__TSEpochSec__ : Number of seconds since the UNIX epoch time of the current timestamp. Obtained by casting the timestamp to a long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: string (nullable = true)\n",
      " |-- TS: timestamp (nullable = true)\n",
      " |-- WindDirectionDegrees: integer (nullable = true)\n",
      " |-- WindSpeedMPH: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- TSEpochSec: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windFormattedDF = unformattedWindDF.selectExpr(\n",
    "    \"Time\",\n",
    "    \"to_timestamp(Time,'yyyy-MM-dd HH:mm:ss') as TS\",\n",
    "    \"cast(WindDirectionDegrees as integer)\",\n",
    "    \"cast(WindSpeedMPH as double)\") \\\n",
    "    .withColumn(\"Date\",col(\"TS\").cast(\"date\")) \\\n",
    "    .withColumn(\"TSEpochSec\",col(\"TS\").cast(\"long\"))\n",
    "\n",
    "windFormattedDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Set Cleanup\n",
    "\n",
    "#### Bad Readings\n",
    "\n",
    "It was noticed that readings where the WindSpeedMPH is 0.0 come with a very high negative WindDirectionDegrees. This could add some noise when resampling so they are converted to 0.\n",
    "\n",
    "There are also some -999.0 values for WindSpeedMPH which seem to be the way the sensor reports some NA or lack of a good reading, they also come with a very high positive WindDirectionDegrees (more than 360). These readings are dropped.\n",
    "\n",
    "Finally a few readings come with a very high WindDirectionDegrees. Again these are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "windNoBadReadingsDF = windFormattedDF \\\n",
    "    .withColumn(\n",
    "        \"WindDirectionDegrees\",\n",
    "        when(col(\"WindDirectionDegrees\")==-737280,0).otherwise(col(\"WindDirectionDegrees\"))) \\\n",
    "    .filter(col(\"WindSpeedMPH\")>=0.0) \\\n",
    "    .filter(col(\"WindDirectionDegrees\").between(0,360))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Daily Sample Set\n",
    "\n",
    "After getting rid of the bad readings the next step is to find the days that don’t have enough readings and then drop them. The threshold used for this notebook is 100 but it is parametrized in the constant section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enoughSamplesPerDayTrans(srcDF, minSampleCount) :\n",
    "    return ( srcDF\n",
    "        .groupBy(\"Date\")\n",
    "        .agg(count(lit(1)).alias(\"SampleCount\"))\n",
    "        .filter(col(\"SampleCount\")>=lit(minSampleCount)) )\n",
    "    \n",
    "windDF = windNoBadReadingsDF.join(\n",
    "    enoughSamplesPerDayTrans(windFormattedDF,MIN_SAMPLE_COUNT),\n",
    "    [\"Date\"],\n",
    "    \"left_semi\")\n",
    "\n",
    "#windDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "\n",
    "The minimum difference between readings of the same day is intended to be 5 minutes but there are situations when the reading for the next 5 min is not taken or the period shifts or in rare occasions is less than 5 min.\n",
    "\n",
    "For our final analysis we would like to find periods of contiguous readings. For this it is easier if the readings are equally spaced in 5 min samples. This will be done by resampling and interpolating the unknown intermediate values from the known ones.\n",
    "\n",
    "Spark currently doesn’t have the same support as R with regards of time series so this has to be done in different stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Current and Next Reading with a Window\n",
    "\n",
    "We need to apply a window function to get the next reading (lead) together with the current one in order to interpolate values between them. The last value of the window will have a null and will be dropped since it is not crucial for this analysis (most likely dropped later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"Date\").orderBy(\"TS\")\n",
    "\n",
    "windPairDF = windDF \\\n",
    "    .withColumn(\"nextWindDirectionDegrees\",lead(\"WindDirectionDegrees\").over(windowSpec)) \\\n",
    "    .withColumn(\"nextWindSpeedMPH\",lead(\"WindSpeedMPH\").over(windowSpec)) \\\n",
    "    .withColumn(\"nextTSEpochSec\",lead(\"TSEpochSec\").over(windowSpec)) \\\n",
    "    .dropna(subset=[\"nextTSEpochSec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Wind Direction With Shortest Angle\n",
    "\n",
    "In case of boundary conditions between 0 and 360 degrees representing the north we will have a problem when doing interpolation. For instance interpolating values between 5 degrees and 355 degrees may take values between these two numbers which will be incorrect. To overcome this problem we have to calculate what is the shortest angle separation between those 2 readings and then calculate the new next direction based on this.\n",
    "\n",
    "The one liner for the shortest angle was obtained from a discussion in stack overflow:\n",
    "\n",
    "$$( ( (\\theta-\\phi) \\% 2\\pi ) + 3\\pi ) \\% 2\\pi ) - \\pi$$\n",
    "\n",
    "Sources:\n",
    "https://stackoverflow.com/questions/2708476/rotation-interpolation/14498790#14498790\n",
    "https://math.stackexchange.com/questions/2144234/interpolating-between-2-angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestAngleExpr(fromAngle, toAngle) :\n",
    "    return ( ( ( ( (toAngle-fromAngle) % lit(360) ) + lit(540) ) % lit(360) ) - lit(180) )\n",
    "\n",
    "def shortestNextAngleExpr(fromAngle, toAngle) :\n",
    "    return fromAngle + shortestAngleExpr(fromAngle, toAngle)\n",
    "\n",
    "windPairFixDirDF = windPairDF \\\n",
    "    .withColumn(\"oldNextWindDeg\",\n",
    "                col(\"nextWindDirectionDegrees\")) \\\n",
    "    .withColumn(\"nextWindDirectionDegrees\",\n",
    "                shortestNextAngleExpr(col(\"WindDirectionDegrees\"), col(\"nextWindDirectionDegrees\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Slopes and y Intercept for Linear Equations\n",
    "\n",
    "The general formula for a linear equation is:\n",
    "\n",
    "$$y=mx+b$$\n",
    "\n",
    "Given 2 points (x1,y1) and (x2,y2) we can calculate the slope (m) and the y intercept (b). \n",
    "\n",
    "The value used as x is TSEpochSec and the y’s are WindDirectionDegrees and nextWindSpeedMPH (We need 2 independent linear equations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Column functions to calculate slope and intercept given 2 points:\n",
    "def slopeExpr(x1,y1,x2,y2) :\n",
    "    return (\n",
    "        (y2-y1) \n",
    "        /(x2-x1)\n",
    "    )\n",
    "\n",
    "def interceptExpr(x1,y1,x2,y2) :\n",
    "    return (\n",
    "      (x1*y2-x2*y1)\n",
    "        /(x1-x2)\n",
    "    )\n",
    "\n",
    "def linearParamTrans(srcDF, paramColName, indColName) :\n",
    "    \"\"\"Transformation to create linear parameters for a set of colums.\"\"\"\n",
    "    return srcDF \\\n",
    "        .withColumn(\"m\"+paramColName,\n",
    "                    slopeExpr(col(indColName),col(paramColName),\n",
    "                              col(\"next\"+indColName),col(\"next\"+paramColName))) \\\n",
    "        .withColumn(\"b\"+paramColName,\n",
    "                    interceptExpr(col(indColName),col(paramColName),\n",
    "                                  col(\"next\"+indColName),col(\"next\"+paramColName)))\n",
    "\n",
    "windLinearDF = windPairFixDirDF \\\n",
    "    .transform(lambda df : linearParamTrans(df,\"WindSpeedMPH\",\"TSEpochSec\")) \\\n",
    "    .transform(lambda df : linearParamTrans(df,\"WindDirectionDegrees\",\"TSEpochSec\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling Points Creation\n",
    "\n",
    "The next step is to create the desired sample points given the range. For instance if the data set contains 2 sets of sample points one at 1:02 and another one at 1:12, the desired sample points after resampling between those two readings will be 1:05 and 1:10.\n",
    "\n",
    "Starting in spark version 2.4.0 there is a sequence operation that can be applied to a 2 columns returning an array. For older versions it is required to create a UDF that provides the same functionality using the python range operator. The data set is then exploded on the column with the array of resampling values. For Sequence the end of the range has to be greater than the start for the python range is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- TS: timestamp (nullable = true)\n",
      " |-- WindDirectionDegrees: integer (nullable = true)\n",
      " |-- WindSpeedMPH: double (nullable = true)\n",
      " |-- TSEpochSec: long (nullable = true)\n",
      " |-- nextWindDirectionDegrees: integer (nullable = true)\n",
      " |-- nextWindSpeedMPH: double (nullable = true)\n",
      " |-- nextTSEpochSec: long (nullable = true)\n",
      " |-- oldNextWindDeg: integer (nullable = true)\n",
      " |-- mWindSpeedMPH: double (nullable = true)\n",
      " |-- bWindSpeedMPH: double (nullable = true)\n",
      " |-- mWindDirectionDegrees: double (nullable = true)\n",
      " |-- bWindDirectionDegrees: double (nullable = true)\n",
      " |-- rangeStart: long (nullable = true)\n",
      " |-- rangeEnd: long (nullable = true)\n",
      " |-- range: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- interTSEpochSec: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import StrictVersion\n",
    "from pyspark.sql.types import ArrayType, LongType\n",
    "\n",
    "# To be more consistent with spark 2.4 the new version of the UDF is just a wrapper for range in python\n",
    "def rangeWrapper(start, end, step) :\n",
    "    # In python the end is not included in the range, in spark/scala it is\n",
    "    return list(range(start,end+1,step))\n",
    "\n",
    "sequenceUdf = udf(rangeWrapper,ArrayType(LongType()))\n",
    "\n",
    "#Finds the next multiple of 5 min to start the range\n",
    "def rangeStartExpr(start,step) :\n",
    "    return (((start+step-lit(1))/step).cast(\"long\")*step)\n",
    "\n",
    "def rangeEndExpr(end) :\n",
    "    return (end-lit(1))\n",
    "\n",
    "# Wrapper for spark version UDF vs native\n",
    "def sequenceVer(start, end, step) :\n",
    "    if StrictVersion(spark.version) >= StrictVersion('2.4.0') :\n",
    "        return sequence(start, end, step)\n",
    "    else :\n",
    "        return sequenceUdf(start, end, step)\n",
    "\n",
    "windResampleDF = windLinearDF \\\n",
    "    .withColumn(\"rangeStart\",rangeStartExpr(col(\"TSEpochSec\"),lit(FIVE_MINUTES_IN_SECS))) \\\n",
    "    .withColumn(\"rangeEnd\",rangeEndExpr(col(\"nextTSEpochSec\"))) \\\n",
    "    .filter(col(\"rangeEnd\")>=col(\"rangeStart\")) \\\n",
    "    .withColumn(\"range\",\n",
    "                sequenceVer(\n",
    "                    col(\"rangeStart\"),\n",
    "                    col(\"rangeEnd\"),\n",
    "                    lit(FIVE_MINUTES_IN_SECS)))\n",
    "\n",
    "windResampleExpDF = windResampleDF \\\n",
    "    .withColumn(\"interTSEpochSec\",explode(col(\"range\")))\n",
    "\n",
    "windResampleExpDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation of Resampling Points\n",
    "\n",
    "At this point the resampling points are located in the column in interTSEpochSec. The next step is to apply the corresponding linear equation to each point to get new interpolated values in columns interWindSpeedMPH and interWindDirectionDegrees. The naming convention of the columns was carefully chosen to be able to replicate the same operations to different columns only using string operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearEquationExpr(x,m,b) :\n",
    "    return (m*x + b)\n",
    "\n",
    "def linearInterpolationTrans(srcDF, paramColName, indColName) :\n",
    "    \"\"\"Transformation to interpolate values.\"\"\"\n",
    "    return srcDF \\\n",
    "        .withColumn(\"inter\"+paramColName,\n",
    "                    linearEquationExpr(col(indColName), col(\"m\"+paramColName), col(\"b\"+paramColName)))\n",
    "\n",
    "interDF = windResampleExpDF \\\n",
    "    .transform(lambda df : linearInterpolationTrans(df,\"WindSpeedMPH\",\"interTSEpochSec\")) \\\n",
    "    .transform(lambda df : linearInterpolationTrans(df,\"WindDirectionDegrees\",\"interTSEpochSec\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Remove this cell it is just for backward compat\n",
    "DF = expDF.selectExpr(\"Date as LocalDate\",\"cast(interTSEpochSec as timestamp) AS Time\",\"interWindSpeedMPH\",\"TS\",\"WindSpeedMPH\")\n",
    "\n",
    "windExtractDF = interDF.selectExpr(\"Date as LocalDate\",\"cast(interTSEpochSec as timestamp) AS Time\",\"interWindSpeedMPH\")\n",
    "#windExtractDF.filter(col(\"LocalDate\")=='2012-03-23').show(545)\n",
    "windExtractDF.show(100)\n",
    "\n",
    "#windExtractDF.coalesce(1).write.option(\"header\",\"true\").csv(\"output\")\n",
    "windExtractDF.sort(\"Time\").coalesce(1).write.option(\"header\",\"true\").csv(\"output/windExtractSortDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Dataset\n",
    "\n",
    "Now it is time to extract only the columns of interest. All the interpolated columns are renamed as if they were the original and intermediate calculations dropped. The interpolated angles could be negative so they have to be converted to a positive version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+------------+--------------------+\n",
      "|      Date|                 TS|TSEpochSec|WindSpeedMPH|WindDirectionDegrees|\n",
      "+----------+-------------------+----------+------------+--------------------+\n",
      "|2012-10-06|2012-10-06 00:05:00|1349499900|         3.0|               149.0|\n",
      "|2012-10-06|2012-10-06 00:10:00|1349500200|         2.5|               149.5|\n",
      "|2012-10-06|2012-10-06 00:15:00|1349500500|         2.0|               150.0|\n",
      "|2012-10-06|2012-10-06 00:20:00|1349500800|         1.0|               150.0|\n",
      "|2012-10-06|2012-10-06 00:25:00|1349501100|         1.0|               150.0|\n",
      "|2012-10-06|2012-10-06 00:30:00|1349501400|         1.5|               149.5|\n",
      "|2012-10-06|2012-10-06 00:35:00|1349501700|         2.0|               149.0|\n",
      "|2012-10-06|2012-10-06 00:40:00|1349502000|         1.5|               149.5|\n",
      "|2012-10-06|2012-10-06 00:45:00|1349502300|         1.0|               150.0|\n",
      "|2012-10-06|2012-10-06 00:50:00|1349502600|         1.0|               149.0|\n",
      "|2012-10-06|2012-10-06 00:55:00|1349502900|         0.0|               150.0|\n",
      "|2012-10-06|2012-10-06 01:00:00|1349503200|         0.0|               150.0|\n",
      "|2012-10-06|2012-10-06 01:05:00|1349503500|         0.0|               150.0|\n",
      "|2012-10-06|2012-10-06 01:10:00|1349503800|         0.0|               150.0|\n",
      "|2012-10-06|2012-10-06 01:15:00|1349504100|         0.0|               150.0|\n",
      "|2012-10-06|2012-10-06 01:20:00|1349504400|         0.0|               150.0|\n",
      "|2012-10-06|2012-10-06 01:25:00|1349504700|         0.0|               150.0|\n",
      "|2012-10-06|2012-10-06 01:30:00|1349505000|         0.0|               150.0|\n",
      "|2012-10-06|2012-10-06 01:35:00|1349505300|         0.0|               149.0|\n",
      "|2012-10-06|2012-10-06 01:40:00|1349505600|         0.0|               150.0|\n",
      "+----------+-------------------+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windCleanDF = interDF.selectExpr(\n",
    "    \"Date\",\n",
    "    \"cast(interTSEpochSec as timestamp) as TS\",\n",
    "    \"interTSEpochSec as TSEpochSec\",\n",
    "    \"interWindSpeedMPH as WindSpeedMPH\",\n",
    "    \"pmod(interWindDirectionDegrees,360) as WindDirectionDegrees\"\n",
    ")\n",
    "windCleanDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daylight Only\n",
    "\n",
    "This analysis is finding the good days for kiteboarding which is a sport that can only be practice during the daylight. It is required to drop any measurements when there is no light to practice the sport.\n",
    "\n",
    "A UDF is used in this case to take advantage of the astral module of python. The use of a UDF for this is not optimal but it is simple. Other approaches could be to use other datasets with daylight information or calculating it natively with formulas but these was more complicated.\n",
    "\n",
    "Once this readings are excluded is a good place to persist the dataframe and create a view for SQL querries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating UDF for sunrise and sunset:\n",
    "getSunriseUdf = udf(lambda date: sensorLoc.sunrise(date),TimestampType())\n",
    "getSunsetUdf = udf(lambda date: sensorLoc.sunset(date),TimestampType())\n",
    "\n",
    "def dayligthExpr(timestamp, date) :\n",
    "     return (col(\"TS\") > getSunriseUdf(col(\"Date\"))) & (col(\"TS\") < getSunsetUdf(col(\"Date\")))\n",
    " \n",
    "dayligthWindDF = windCleanDF.filter(dayligthExpr(col(\"TS\"), col(\"Date\")))\n",
    "\n",
    "dayligthWindDF.persist()\n",
    "dayligthWindDF.createOrReplaceTempView(\"dayligthWindTable\")\n",
    "#dayligthWindDF.show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Wind Roses\n",
    "\n",
    "The first analysis done after cleaning the date was to get a windrose diagram to see the wind speed and direction distribution for the years we are analyzing. It is divided by month since we are interested on finding the good season for kiting.\n",
    "\n",
    "A windrose diagram is just a 2D histogram in polar coordinates. One of the axis will be a direction range in angles and the other will be the ranges of wind magnitude. This is the SQL query that will create the normalized 2D histogram (Converted to pandas dataframe for plotting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input 'FROM' expecting <EOF>(line 3, pos 2)\\n\\n== SQL ==\\n\\n  SELECT *\\n  FROM (\\n--^^^\\n    SELECT\\n      month(Date) as Month,\\n      (CAST(((WindDirectionDegrees*16)/360) + 0.5 AS INT) ) % 16 AS WindDirectionGroup,\\n      CASE\\n        WHEN WindSpeedMPH < 15 THEN 'r_0_15'\\n        WHEN WindSpeedMPH < 20 THEN 'r_15_20'\\n        WHEN WindSpeedMPH < 25 THEN 'r_20_25'\\n        ELSE 'r_25_inf'\\n      END AS WindSpeedGroup\\n    FROM dayligthWindTable\\n  )\\n  PIVOT (\\n    COUNT(1)\\n    FOR WindSpeedGroup IN ('r_0_15', 'r_15_20', 'r_20_25', 'r_25_inf')\\n  )\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/spark/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input 'FROM' expecting <EOF>(line 3, pos 2)\n\n== SQL ==\n\n  SELECT *\n  FROM (\n--^^^\n    SELECT\n      month(Date) as Month,\n      (CAST(((WindDirectionDegrees*16)/360) + 0.5 AS INT) ) % 16 AS WindDirectionGroup,\n      CASE\n        WHEN WindSpeedMPH < 15 THEN 'r_0_15'\n        WHEN WindSpeedMPH < 20 THEN 'r_15_20'\n        WHEN WindSpeedMPH < 25 THEN 'r_20_25'\n        ELSE 'r_25_inf'\n      END AS WindSpeedGroup\n    FROM dayligthWindTable\n  )\n  PIVOT (\n    COUNT(1)\n    FOR WindSpeedGroup IN ('r_0_15', 'r_15_20', 'r_20_25', 'r_25_inf')\n  )\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:239)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:115)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641)\n\tat sun.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a2bdfc20a8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mFOR\u001b[0m \u001b[0mWindSpeedGroup\u001b[0m \u001b[0mIN\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r_0_15'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r_15_20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r_20_25'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r_25_inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   )\n\u001b[0;32m---> 19\u001b[0;31m \"\"\")\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \"\"\"\n",
      "\u001b[0;32m~/Documents/spark/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \"\"\"\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input 'FROM' expecting <EOF>(line 3, pos 2)\\n\\n== SQL ==\\n\\n  SELECT *\\n  FROM (\\n--^^^\\n    SELECT\\n      month(Date) as Month,\\n      (CAST(((WindDirectionDegrees*16)/360) + 0.5 AS INT) ) % 16 AS WindDirectionGroup,\\n      CASE\\n        WHEN WindSpeedMPH < 15 THEN 'r_0_15'\\n        WHEN WindSpeedMPH < 20 THEN 'r_15_20'\\n        WHEN WindSpeedMPH < 25 THEN 'r_20_25'\\n        ELSE 'r_25_inf'\\n      END AS WindSpeedGroup\\n    FROM dayligthWindTable\\n  )\\n  PIVOT (\\n    COUNT(1)\\n    FOR WindSpeedGroup IN ('r_0_15', 'r_15_20', 'r_20_25', 'r_25_inf')\\n  )\\n\""
     ]
    }
   ],
   "source": [
    "normedWindRoseDF = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    SELECT\n",
    "      month(Date) as Month,\n",
    "      (CAST(((WindDirectionDegrees*16)/360) + 0.5 AS INT) ) % 16 AS WindDirectionGroup,\n",
    "      CASE\n",
    "        WHEN WindSpeedMPH < 15 THEN 'r_0_15'\n",
    "        WHEN WindSpeedMPH < 20 THEN 'r15_20'\n",
    "        WHEN WindSpeedMPH < 25 THEN 'r_20_25'\n",
    "        ELSE 'r_25_inf'\n",
    "      END AS WindSpeedGroup\n",
    "    FROM dayligthWindTable\n",
    "  )\n",
    "  PIVOT (\n",
    "    COUNT(1)\n",
    "    FOR WindSpeedGroup IN ('r_0_15', 'r_15_20', 'r_20_25', 'r_25_inf')\n",
    "  )\n",
    "\"\"\")\n",
    "\n",
    "\"\"\"\n",
    "normedWindRoseDF = spark.sql(\n",
    "SELECT\n",
    "  Month,\n",
    "  WindDirectionGroup,\n",
    "  IFNULL(r_0_15,0)/Total AS `0 to 15`,\n",
    "  IFNULL(r_15_20,0)/Total AS `15 to 20`,\n",
    "  IFNULL(r_20_25,0)/Total AS `20 to 25`,\n",
    "  IFNULL(r_25_inf,0)/Total AS `25 or over`\n",
    "FROM (\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    SELECT\n",
    "      month(Date) as Month,\n",
    "      (CAST(((WindDirectionDegrees*16)/360) + 0.5 AS INT) ) % 16 AS WindDirectionGroup,\n",
    "      CASE\n",
    "        WHEN WindSpeedMPH < 15 THEN 'r_0_15'\n",
    "        WHEN WindSpeedMPH < 20 THEN 'r_15_20'\n",
    "        WHEN WindSpeedMPH < 25 THEN 'r_20_25'\n",
    "        ELSE 'r_25_inf'\n",
    "      END AS WindSpeedGroup\n",
    "    FROM dayligthWindTable\n",
    "  )\n",
    "  PIVOT (\n",
    "    COUNT(1)\n",
    "    FOR WindSpeedGroup IN ('r_0_15', 'r_15_20', 'r_20_25', 'r_25_inf')\n",
    "  )\n",
    ") AS windRoseTable\n",
    "JOIN (\n",
    "  SELECT month(Date) AS MGroup, COUNT(1) as Total\n",
    "  FROM dayligthWindTable\n",
    "  GROUP BY month(Date)\n",
    ") AS totalByMonthTable\n",
    "ON windRoseTable.Month = totalByMonthTable.MGroup\n",
    "ORDER BY Month DESC, WindDirectionGroup\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "normedWindRoseDF.show()\n",
    "\n",
    "#normedWindRosePD = normedWindRoseDF.toPandas()\n",
    "\n",
    "#spark.catalog.dropTempView(\"windRoseTable\")\n",
    "#spark.catalog.dropTempView(\"dayligthWindTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
