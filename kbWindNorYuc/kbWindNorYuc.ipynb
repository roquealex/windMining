{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Analysis Of Northern Yucatan Coast for Kiteboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries\n",
    "\n",
    "This is the list of libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Dataframe:\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "# Daylight UDF:\n",
    "import astral\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Plot related\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import calmap\n",
    "\n",
    "# Math\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Access of env vars\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "Spark version 2.4.0 or above is required since it uses sequence and PIVOT on SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import StrictVersion\n",
    "assert StrictVersion(spark.version) >= StrictVersion('2.4.0'), \"Notebook requires spark 2.4.0 or above\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIVE_MINUTES_IN_SECS = (5*60) # Seconds\n",
    "TWO_HOURS_IN_5MINS = (2*60)//5 #how many 5 min periods in 2 hours\n",
    "MIN_SAMPLE_COUNT = 100\n",
    "DATABRICKS_ENV = 'DATABRICKS_RUNTIME_VERSION' in os.environ.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, f):\n",
    "    return f(self)\n",
    "\n",
    "DataFrame.transform = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "The weather station (PWS) used for this analysis is CINVESTAV Telchac located in the Northern Yucantan coast. Coordinates of the station are latitude 21.341108 and longitude -89.305756. This is a single PWS project so spark will be configured in the timeszone of the project and there will be a location object for the PWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwsID = 'IYUCATNT2'\n",
    "pwsTz = 'America/Merida'\n",
    "sensorLoc = astral.Location((\"Telchac\", \"Mexico\", 21.341108, -89.305756, pwsTz, 0))\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", pwsTz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information was gathered from Weather Underground web site that provides a free API call to gather the information of a particular day. The weather station ID for this location is IYUCATNT2. The response comes almost in a CSV format except for the insertion of br html tags.\n",
    "\n",
    "For instance the following URL will provide the data for January 1st, 2016:\n",
    "\n",
    "https://www.wunderground.com/weatherstation/WXDailyHistory.asp?ID=IYUCATNT2&month=1&day=1&year=2016&format=1\n",
    "\n",
    "A crawler script (with some wait between request) was created to dump the sensor information and do the simple task of converting it to a CSV (removing the inserted tags). The data is organized in subdirectories for the station ID, year and month for easy management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks location:\n",
    "#fileStorePath = \"/FileStore/tables/test1\"\n",
    "    \n",
    "fileStorePath = \"./\"\n",
    "\n",
    "unformattedWindDF = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"{}/dailyHistory/{}/*/*/*.csv\".format(fileStorePath,pwsID))\n",
    "\n",
    "#unformattedWindDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The sensor has historical data only from the middle of 2008 so the range for this analysis is from 2009 to 2018.\n",
    "\n",
    "The table that Weather Underground returns has a lot of columns that represent measurements from the PWS like pressure, temperature, dew point, etc. but for this analysis we only focused on the following:\n",
    "\n",
    "__Time__ : This is the timestamp at which the sample is taken in the local timezone of the PWS. There is another column that has the time in UTC but not used in this analysis. This column is kept as a string for comparison.\n",
    "\n",
    "__WindDirectionDegrees__ : This is the wind direction in degrees. The cardinal directions map as follow: North is 0, East is 90, South is 180 and West is 270. This is casted to integer.\n",
    "\n",
    "__WindSpeedMPH__ : Wind speed measured in miles per hour (mph). There is also a gust wind speed column but not used in this analysis. This is casted to a double.\n",
    "\n",
    "The following columns are derived from the previous:\n",
    "\n",
    "__TS__ : Conversion of the _Time_ string to a timestamp. Casting from string to timestamp was the best method posible found to handle timestamps in a the PWS timezone from the local timezone in spark.\n",
    "\n",
    "__Date__ : Extraction of the date portion of the timestamp.\n",
    "\n",
    "__TSEpochSec__ : Number of seconds since the UNIX epoch time of the current timestamp. Obtained by casting the timestamp to a long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: string (nullable = true)\n",
      " |-- TS: timestamp (nullable = true)\n",
      " |-- WindDirectionDegrees: integer (nullable = true)\n",
      " |-- WindSpeedMPH: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- TSEpochSec: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windFormattedDF = unformattedWindDF.selectExpr(\n",
    "    \"Time\",\n",
    "    \"to_timestamp(Time,'yyyy-MM-dd HH:mm:ss') as TS\",\n",
    "    \"cast(WindDirectionDegrees as integer)\",\n",
    "    \"cast(WindSpeedMPH as double)\") \\\n",
    "    .withColumn(\"Date\",col(\"TS\").cast(\"date\")) \\\n",
    "    .withColumn(\"TSEpochSec\",col(\"TS\").cast(\"long\"))\n",
    "\n",
    "windFormattedDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Set Cleanup\n",
    "\n",
    "#### Bad Readings\n",
    "\n",
    "It was noticed that readings where the WindSpeedMPH is 0.0 come with a very high negative WindDirectionDegrees. This could add some noise when resampling so they are converted to 0.\n",
    "\n",
    "There are also some -999.0 values for WindSpeedMPH which seem to be the way the sensor reports some NA or lack of a good reading, they also come with a very high positive WindDirectionDegrees (more than 360). These readings are dropped.\n",
    "\n",
    "Finally a few readings come with a very high WindDirectionDegrees. Again these are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "windNoBadReadingsDF = windFormattedDF \\\n",
    "    .withColumn(\n",
    "        \"WindDirectionDegrees\",\n",
    "        when(col(\"WindDirectionDegrees\")==-737280,0).otherwise(col(\"WindDirectionDegrees\"))) \\\n",
    "    .filter(col(\"WindSpeedMPH\")>=0.0) \\\n",
    "    .filter(col(\"WindDirectionDegrees\").between(0,360))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Daily Sample Set\n",
    "\n",
    "After getting rid of the bad readings the next step is to find the days that donâ€™t have enough readings and then drop them. The threshold used for this notebook is 100 but it is parametrized in the constant section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enoughSamplesPerDayTrans(srcDF, minSampleCount) :\n",
    "    return ( srcDF\n",
    "        .groupBy(\"Date\")\n",
    "        .agg(count(lit(1)).alias(\"SampleCount\"))\n",
    "        .filter(col(\"SampleCount\")>=lit(minSampleCount)) )\n",
    "    \n",
    "windDF = windNoBadReadingsDF.join(\n",
    "    enoughSamplesPerDayTrans(windFormattedDF,MIN_SAMPLE_COUNT),\n",
    "    [\"Date\"],\n",
    "    \"left_semi\")\n",
    "\n",
    "#windDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "\n",
    "The minimum difference between readings of the same day is intended to be 5 minutes but there are situations when the reading for the next 5 min is not taken or the period shifts or in rare occasions is less than 5 min.\n",
    "\n",
    "For our final analysis we would like to find periods of contiguous readings. For this it is easier if the readings are equally spaced in 5 min samples. This will be done by resampling and interpolating the unknown intermediate values from the known ones.\n",
    "\n",
    "Spark currently doesnâ€™t have the same support as R with regards of time series so this has to be done in different stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Current and Next Reading with a Window\n",
    "\n",
    "We need to apply a window function to get the next reading (lead) together with the current one in order to interpolate values between them. The last value of the window will have a null and will be dropped since it is not crucial for this analysis (most likely dropped later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"Date\").orderBy(\"TS\")\n",
    "\n",
    "windPairDF = windDF \\\n",
    "    .withColumn(\"nextWindDirectionDegrees\",lead(\"WindDirectionDegrees\").over(windowSpec)) \\\n",
    "    .withColumn(\"nextWindSpeedMPH\",lead(\"WindSpeedMPH\").over(windowSpec)) \\\n",
    "    .withColumn(\"nextTSEpochSec\",lead(\"TSEpochSec\").over(windowSpec)) \\\n",
    "    .dropna(subset=[\"nextTSEpochSec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Wind Direction With Shortest Angle\n",
    "\n",
    "In case of boundary conditions between 0 and 360 degrees representing the north we will have a problem when doing interpolation. For instance interpolating values between 5 degrees and 355 degrees may take values between these two numbers which will be incorrect. To overcome this problem we have to calculate what is the shortest angle separation between those 2 readings and then calculate the new next direction based on this.\n",
    "\n",
    "The one liner for the shortest angle was obtained from a discussion in stack overflow:\n",
    "\n",
    "$$( ( (\\theta-\\phi) \\% 2\\pi ) + 3\\pi ) \\% 2\\pi ) - \\pi$$\n",
    "\n",
    "Sources:\n",
    "https://stackoverflow.com/questions/2708476/rotation-interpolation/14498790#14498790\n",
    "https://math.stackexchange.com/questions/2144234/interpolating-between-2-angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestAngleExpr(fromAngle, toAngle) :\n",
    "    return ( ( ( ( (toAngle-fromAngle) % lit(360) ) + lit(540) ) % lit(360) ) - lit(180) )\n",
    "\n",
    "def shortestNextAngleExpr(fromAngle, toAngle) :\n",
    "    return fromAngle + shortestAngleExpr(fromAngle, toAngle)\n",
    "\n",
    "windPairFixDirDF = windPairDF \\\n",
    "    .withColumn(\"oldNextWindDeg\",\n",
    "                col(\"nextWindDirectionDegrees\")) \\\n",
    "    .withColumn(\"nextWindDirectionDegrees\",\n",
    "                shortestNextAngleExpr(col(\"WindDirectionDegrees\"), col(\"nextWindDirectionDegrees\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Slopes and y Intercept for Linear Equations\n",
    "\n",
    "The general formula for a linear equation is:\n",
    "\n",
    "$$y=mx+b$$\n",
    "\n",
    "Given 2 points (x1,y1) and (x2,y2) we can calculate the slope (m) and the y intercept (b). \n",
    "\n",
    "The value used as x is TSEpochSec and the yâ€™s are WindDirectionDegrees and nextWindSpeedMPH (We need 2 independent linear equations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Column functions to calculate slope and intercept given 2 points:\n",
    "def slopeExpr(x1,y1,x2,y2) :\n",
    "    return (\n",
    "        (y2-y1) \n",
    "        /(x2-x1)\n",
    "    )\n",
    "\n",
    "def interceptExpr(x1,y1,x2,y2) :\n",
    "    return (\n",
    "      (x1*y2-x2*y1)\n",
    "        /(x1-x2)\n",
    "    )\n",
    "\n",
    "def linearParamTrans(srcDF, paramColName, indColName) :\n",
    "    \"\"\"Transformation to create linear parameters for a set of colums.\"\"\"\n",
    "    return srcDF \\\n",
    "        .withColumn(\"m\"+paramColName,\n",
    "                    slopeExpr(col(indColName),col(paramColName),\n",
    "                              col(\"next\"+indColName),col(\"next\"+paramColName))) \\\n",
    "        .withColumn(\"b\"+paramColName,\n",
    "                    interceptExpr(col(indColName),col(paramColName),\n",
    "                                  col(\"next\"+indColName),col(\"next\"+paramColName)))\n",
    "\n",
    "windLinearDF = windPairFixDirDF \\\n",
    "    .transform(lambda df : linearParamTrans(df,\"WindSpeedMPH\",\"TSEpochSec\")) \\\n",
    "    .transform(lambda df : linearParamTrans(df,\"WindDirectionDegrees\",\"TSEpochSec\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling Points Creation\n",
    "\n",
    "The next step is to create the desired sample points given the range. For instance if the data set contains 2 sets of sample points one at 1:02 and another one at 1:12, the desired sample points after resampling between those two readings will be 1:05 and 1:10.\n",
    "\n",
    "Starting in spark version 2.4.0 there is a sequence operation that can be applied to a 2 columns returning an array. For older versions it is required to create a UDF that provides the same functionality using the python range operator. The data set is then exploded on the column with the array of resampling values. For Sequence the end of the range has to be greater than the start for the python range is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- TS: timestamp (nullable = true)\n",
      " |-- WindDirectionDegrees: integer (nullable = true)\n",
      " |-- WindSpeedMPH: double (nullable = true)\n",
      " |-- TSEpochSec: long (nullable = true)\n",
      " |-- nextWindDirectionDegrees: integer (nullable = true)\n",
      " |-- nextWindSpeedMPH: double (nullable = true)\n",
      " |-- nextTSEpochSec: long (nullable = true)\n",
      " |-- oldNextWindDeg: integer (nullable = true)\n",
      " |-- mWindSpeedMPH: double (nullable = true)\n",
      " |-- bWindSpeedMPH: double (nullable = true)\n",
      " |-- mWindDirectionDegrees: double (nullable = true)\n",
      " |-- bWindDirectionDegrees: double (nullable = true)\n",
      " |-- rangeStart: long (nullable = true)\n",
      " |-- rangeEnd: long (nullable = true)\n",
      " |-- range: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- interTSEpochSec: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import StrictVersion\n",
    "from pyspark.sql.types import ArrayType, LongType\n",
    "\n",
    "# To be more consistent with spark 2.4 the new version of the UDF is just a wrapper for range in python\n",
    "def rangeWrapper(start, end, step) :\n",
    "    # In python the end is not included in the range, in spark/scala it is\n",
    "    return list(range(start,end+1,step))\n",
    "\n",
    "sequenceUdf = udf(rangeWrapper,ArrayType(LongType()))\n",
    "\n",
    "#Finds the next multiple of 5 min to start the range\n",
    "def rangeStartExpr(start,step) :\n",
    "    return (((start+step-lit(1))/step).cast(\"long\")*step)\n",
    "\n",
    "def rangeEndExpr(end) :\n",
    "    return (end-lit(1))\n",
    "\n",
    "# Wrapper for spark version UDF vs native\n",
    "def sequenceVer(start, end, step) :\n",
    "    if StrictVersion(spark.version) >= StrictVersion('2.4.0') :\n",
    "        return sequence(start, end, step)\n",
    "    else :\n",
    "        return sequenceUdf(start, end, step)\n",
    "\n",
    "windResampleDF = windLinearDF \\\n",
    "    .withColumn(\"rangeStart\",rangeStartExpr(col(\"TSEpochSec\"),lit(FIVE_MINUTES_IN_SECS))) \\\n",
    "    .withColumn(\"rangeEnd\",rangeEndExpr(col(\"nextTSEpochSec\"))) \\\n",
    "    .filter(col(\"rangeEnd\")>=col(\"rangeStart\")) \\\n",
    "    .withColumn(\"range\",\n",
    "                sequenceVer(\n",
    "                    col(\"rangeStart\"),\n",
    "                    col(\"rangeEnd\"),\n",
    "                    lit(FIVE_MINUTES_IN_SECS)))\n",
    "\n",
    "windResampleExpDF = windResampleDF \\\n",
    "    .withColumn(\"interTSEpochSec\",explode(col(\"range\")))\n",
    "\n",
    "windResampleExpDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation of Resampling Points\n",
    "\n",
    "At this point the resampling points are located in the column in interTSEpochSec. The next step is to apply the corresponding linear equation to each point to get new interpolated values in columns interWindSpeedMPH and interWindDirectionDegrees. The naming convention of the columns was carefully chosen to be able to replicate the same operations to different columns only using string operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearEquationExpr(x,m,b) :\n",
    "    return (m*x + b)\n",
    "\n",
    "def linearInterpolationTrans(srcDF, paramColName, indColName) :\n",
    "    \"\"\"Transformation to interpolate values.\"\"\"\n",
    "    return srcDF \\\n",
    "        .withColumn(\"inter\"+paramColName,\n",
    "                    linearEquationExpr(col(indColName), col(\"m\"+paramColName), col(\"b\"+paramColName)))\n",
    "\n",
    "interDF = windResampleExpDF \\\n",
    "    .transform(lambda df : linearInterpolationTrans(df,\"WindSpeedMPH\",\"interTSEpochSec\")) \\\n",
    "    .transform(lambda df : linearInterpolationTrans(df,\"WindDirectionDegrees\",\"interTSEpochSec\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------+\n",
      "| LocalDate|               Time|interWindSpeedMPH|\n",
      "+----------+-------------------+-----------------+\n",
      "|2012-10-06|2012-10-06 00:05:00|              3.0|\n",
      "|2012-10-06|2012-10-06 00:10:00|              2.5|\n",
      "|2012-10-06|2012-10-06 00:15:00|              2.0|\n",
      "|2012-10-06|2012-10-06 00:20:00|              1.0|\n",
      "|2012-10-06|2012-10-06 00:25:00|              1.0|\n",
      "|2012-10-06|2012-10-06 00:30:00|              1.5|\n",
      "|2012-10-06|2012-10-06 00:35:00|              2.0|\n",
      "|2012-10-06|2012-10-06 00:40:00|              1.5|\n",
      "|2012-10-06|2012-10-06 00:45:00|              1.0|\n",
      "|2012-10-06|2012-10-06 00:50:00|              1.0|\n",
      "|2012-10-06|2012-10-06 00:55:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:00:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:05:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:10:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:15:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:20:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:25:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:30:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:35:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:40:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:45:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:50:00|              0.0|\n",
      "|2012-10-06|2012-10-06 01:55:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:00:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:05:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:10:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:15:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:20:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:25:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:30:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:35:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:40:00|              0.0|\n",
      "|2012-10-06|2012-10-06 02:45:00|              2.0|\n",
      "|2012-10-06|2012-10-06 02:50:00|              2.0|\n",
      "|2012-10-06|2012-10-06 02:55:00|              1.5|\n",
      "|2012-10-06|2012-10-06 03:00:00|              1.0|\n",
      "|2012-10-06|2012-10-06 03:05:00|              0.5|\n",
      "|2012-10-06|2012-10-06 03:10:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:15:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:20:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:25:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:30:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:35:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:40:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:45:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:50:00|              0.0|\n",
      "|2012-10-06|2012-10-06 03:55:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:00:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:05:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:10:00|              0.5|\n",
      "|2012-10-06|2012-10-06 04:15:00|              1.0|\n",
      "|2012-10-06|2012-10-06 04:20:00|              1.0|\n",
      "|2012-10-06|2012-10-06 04:25:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:30:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:35:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:40:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:45:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:50:00|              0.0|\n",
      "|2012-10-06|2012-10-06 04:55:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:00:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:05:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:10:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:15:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:20:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:25:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:30:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:35:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:40:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:45:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:50:00|              0.0|\n",
      "|2012-10-06|2012-10-06 05:55:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:00:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:05:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:10:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:15:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:20:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:25:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:30:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:35:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:40:00|              0.5|\n",
      "|2012-10-06|2012-10-06 06:45:00|              1.0|\n",
      "|2012-10-06|2012-10-06 06:50:00|              0.0|\n",
      "|2012-10-06|2012-10-06 06:55:00|              1.0|\n",
      "|2012-10-06|2012-10-06 07:00:00|              1.0|\n",
      "|2012-10-06|2012-10-06 07:05:00|              1.0|\n",
      "|2012-10-06|2012-10-06 07:10:00|              0.5|\n",
      "|2012-10-06|2012-10-06 07:15:00|              0.0|\n",
      "|2012-10-06|2012-10-06 07:20:00|              0.0|\n",
      "|2012-10-06|2012-10-06 07:25:00|              0.0|\n",
      "|2012-10-06|2012-10-06 07:30:00|              0.0|\n",
      "|2012-10-06|2012-10-06 07:35:00|              2.0|\n",
      "|2012-10-06|2012-10-06 07:40:00|              2.5|\n",
      "|2012-10-06|2012-10-06 07:45:00|              3.0|\n",
      "|2012-10-06|2012-10-06 07:50:00|              3.0|\n",
      "|2012-10-06|2012-10-06 07:55:00|              3.0|\n",
      "|2012-10-06|2012-10-06 08:00:00|              3.5|\n",
      "|2012-10-06|2012-10-06 08:05:00|              4.0|\n",
      "|2012-10-06|2012-10-06 08:10:00|              5.0|\n",
      "|2012-10-06|2012-10-06 08:15:00|              5.0|\n",
      "|2012-10-06|2012-10-06 08:20:00|              4.5|\n",
      "+----------+-------------------+-----------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "windExtractDF = interDF.selectExpr(\"Date as LocalDate\",\"cast(interTSEpochSec as timestamp) AS Time\",\"interWindSpeedMPH\")\n",
    "#windExtractDF.filter(col(\"LocalDate\")=='2012-03-23').show(545)\n",
    "windExtractDF.show(100)\n",
    "\n",
    "#windExtractDF.coalesce(1).write.option(\"header\",\"true\").csv(\"output\")\n",
    "windExtractDF.sort(\"Time\").coalesce(1).write.option(\"header\",\"true\").csv(\"output/windExtractSortDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Dataset\n",
    "\n",
    "Now it is time to extract only the columns of interest. All the interpolated columns are renamed as if they were the original and intermediate calculations dropped. The interpolated angles could be negative so they have to be converted to a positive version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windCleanDF = interDF.selectExpr(\n",
    "    \"Date\",\n",
    "    \"cast(interTSEpochSec as timestamp) as TS\",\n",
    "    \"interTSEpochSec as TSEpochSec\",\n",
    "    \"interWindSpeedMPH as WindSpeedMPH\",\n",
    "    \"pmod(interWindDirectionDegrees,360) as WindDirectionDegrees\"\n",
    ")\n",
    "windCleanDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daylight Only\n",
    "\n",
    "This analysis is finding the good days for kiteboarding which is a sport that can only be practice during the daylight. It is required to drop any measurements when there is no light to practice the sport.\n",
    "\n",
    "A UDF is used in this case to take advantage of the astral module of python. The use of a UDF for this is not optimal but it is simple. Other approaches could be to use other datasets with daylight information or calculating it natively with formulas but these was more complicated.\n",
    "\n",
    "Once this readings are excluded is a good place to persist the dataframe and create a view for SQL querries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating UDF for sunrise and sunset:\n",
    "getSunriseUdf = udf(lambda date: sensorLoc.sunrise(date),TimestampType())\n",
    "getSunsetUdf = udf(lambda date: sensorLoc.sunset(date),TimestampType())\n",
    "\n",
    "def dayligthExpr(timestamp, date) :\n",
    "     return (col(\"TS\") > getSunriseUdf(col(\"Date\"))) & (col(\"TS\") < getSunsetUdf(col(\"Date\")))\n",
    " \n",
    "dayligthWindDF = windCleanDF.filter(dayligthExpr(col(\"TS\"), col(\"Date\")))\n",
    "\n",
    "dayligthWindDF.persist()\n",
    "dayligthWindDF.createOrReplaceTempView(\"dayligthWindTable\")\n",
    "#dayligthWindDF.show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Wind Roses\n",
    "\n",
    "The first analysis done after cleaning the date was to get a windrose diagram to see the wind speed and direction distribution for the years we are analyzing. It is divided by month since we are interested on finding the good season for kiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for a 2D Histogram\n",
    "\n",
    "A windrose diagram is just a 2D histogram in polar coordinates. One of the axis will be a direction range in angles and the other will be the ranges of wind magnitude. The wind speed ranges chosen are meaningful for the sport:\n",
    "\n",
    "__Less than 15 MPH__ : Wind is low. It is feasible to kite in a 2 digit low wind condition with the use of foilboards, surfboards or raceboards but they are not as portable and probably nobody wants to fly away to kite in low winds.\n",
    "\n",
    "__\\[15,20\\) MPH__ : This is a comfortable wind range to kite with a big kite (12m for the average kiter).\n",
    "\n",
    "__\\[20,25\\) MPH__ : This is a comfortable wind range to kite with a medium size kite (10m or 9m for the average kiter).\n",
    "\n",
    "__25 MPH or over__ : This is a range where a smaller kite would be required (7m or 8m). No longer advisable for beginners.\n",
    "\n",
    "This is the SQL query that will create the normalized 2D histogram (Converted to pandas dataframe for plotting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normedWindRoseDF = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  Month,\n",
    "  WindDirectionGroup,\n",
    "  IFNULL(r_0_15,0)/Total AS `0 to 15`,\n",
    "  IFNULL(r_15_20,0)/Total AS `15 to 20`,\n",
    "  IFNULL(r_20_25,0)/Total AS `20 to 25`,\n",
    "  IFNULL(r_25_inf,0)/Total AS `25 or over`\n",
    "FROM (\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    SELECT\n",
    "      month(Date) as Month,\n",
    "      (CAST(((WindDirectionDegrees*16)/360) + 0.5 AS INT) ) % 16 AS WindDirectionGroup,\n",
    "      CASE\n",
    "        WHEN WindSpeedMPH < 15 THEN 'r_0_15'\n",
    "        WHEN WindSpeedMPH < 20 THEN 'r_15_20'\n",
    "        WHEN WindSpeedMPH < 25 THEN 'r_20_25'\n",
    "        ELSE 'r_25_inf'\n",
    "      END AS WindSpeedGroup\n",
    "    FROM dayligthWindTable\n",
    "  )\n",
    "  PIVOT (\n",
    "    COUNT(1)\n",
    "    FOR WindSpeedGroup IN ('r_0_15', 'r_15_20', 'r_20_25', 'r_25_inf')\n",
    "  )\n",
    ") AS windRoseTable\n",
    "JOIN (\n",
    "  SELECT month(Date) AS MGroup, COUNT(1) as Total\n",
    "  FROM dayligthWindTable\n",
    "  GROUP BY month(Date)\n",
    ") AS totalByMonthTable\n",
    "ON windRoseTable.Month = totalByMonthTable.MGroup\n",
    "ORDER BY Month DESC, WindDirectionGroup\n",
    "\"\"\")\n",
    "normedWindRosePD = normedWindRoseDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Function\n",
    "\n",
    "The plots are organized in 3 rows by 4 columns. The windrose is created by using a bar plot with polar coordinates. Some font size had to be decreased for the databricks notebook otherwise it looks fine in jupyter. There are native python libraries that support plotting of wind roses but they receive the collection of measurements instead of the final histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWindRose(windRosePD, ranges) :\n",
    "    windRoseGroupsPD = windRosePD.groupby('Month')\n",
    "    fig, axes = plt.subplots(3,4, figsize=(14, 14), subplot_kw=dict(polar=True))\n",
    "    for rown, axrow in enumerate(axes) :\n",
    "        for coln, ax in enumerate(axrow):\n",
    "            monthNum = rown*len(axrow)+coln+1\n",
    "            ax.set_theta_zero_location('N')\n",
    "            ax.set_theta_direction('clockwise')\n",
    "            ax.set_title(calendar.month_name[monthNum],loc='left')\n",
    "            ax.set_xticklabels(['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'])\n",
    "            try:\n",
    "                windRoseGroupPD = windRoseGroupsPD.get_group(monthNum)\n",
    "                for i, (name,color) in enumerate(ranges) :\n",
    "                    ax.bar(\n",
    "                        2*(math.pi)*windRoseGroupPD['WindDirectionGroup']/16,\n",
    "                        windRoseGroupPD[name],\n",
    "                        width=0.80*2*(math.pi)/16, #Radians\n",
    "                        bottom= 0 if i == 0 else windRoseGroupPD[list(map(lambda x : x[0],ranges[0:i]))].sum(axis=1),\n",
    "                        color=color,\n",
    "                        #edgecolor='None',\n",
    "                        label=name,\n",
    "                        linewidth=0)\n",
    "                    ax.legend(loc='lower center',ncol=2, bbox_to_anchor=(0.5, -0.35), fontsize='small')\n",
    "                ax.set_yticklabels(map(lambda x : \"{:.0%}\".format(x),ax.get_yticks()), fontdict={'fontsize': 'x-small'}, minor=False)\n",
    "            except KeyError as keyErr:\n",
    "                print('WARNING: No entries for month',keyErr)\n",
    "    return (fig, axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Plots\n",
    "\n",
    "The function receives an array of tuples that has the columns that we want to plot from the dataframe and the color that we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangesSpec = [\n",
    "    ('0 to 15','Yellow'),\n",
    "    ('15 to 20','Orange'),\n",
    "    ('20 to 25','Red'),\n",
    "    ('25 or over','Maroon')\n",
    "]\n",
    "fig, axes = plotWindRose(normedWindRosePD,rangesSpec)\n",
    "if DATABRICKS_ENV : display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wind Rose Analysis\n",
    "\n",
    "There are 2 dominant wind directions north-east and south-east. NE winds are stronger overall compared to the SE ones. This is good news since the NE direction creates a side-onshore direction excellent for kiting in the north shore of Yucatan.\n",
    "\n",
    "Visually we can observe that around september we get the lowest winds. The winter-spring months seem to have the highest winds overall. May and April seem to be a good solid months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search of Windy Days\n",
    "\n",
    "The information from the wind rose is a good quick outlook of how to the winds distribute along the year but it doesnâ€™t answer the question of how many good days for kiting a month has.\n",
    "\n",
    "Everyday is scanned to look for a period of at least 2 hours of sustained winds in the ranges of 15, 20 and 25 mph. The fact that each day was resampled at 5 min intervals greatly simplifies this analysis since it becomes a grouping and counting problem rather than finding time differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for Periods of Time Longer Than 2 Hours\n",
    "\n",
    "First step is to look for the amount of time the wind speed was equal or greater than 15, 20 and 25 mph respectively. A time range is only good if we have at least 2 hours of contiguous wind over the desired speed otherwise the range is discarded. If there are multiple periods going over the 2 hours threshold then they are added together.\n",
    "\n",
    "In terms of SQL we filter when the speed is greater than the desired speed and we use a scaled ROW_NUMBER window function and the TSEpochSec to find contiguous measurements (A disruption on the difference tells us it is a different group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windyDaysQuery(windThreshold, timeThreshold) :\n",
    "    queryStr = \"\"\"\n",
    "    SELECT Date, SUM(DayGroupCount) AS Total{windTh}\n",
    "    FROM (\n",
    "      SELECT Date, DayGroup, COUNT(1) as DayGroupCount\n",
    "      FROM (\n",
    "        SELECT *,\n",
    "          TSEpochSec - {fiveMin}*(ROW_NUMBER() OVER (PARTITION BY Date ORDER BY TSEpochSec))\n",
    "            AS DayGroup\n",
    "        FROM (\n",
    "          SELECT *\n",
    "          FROM dayligthWindTable\n",
    "          WHERE WindSpeedMPH >= {windTh}\n",
    "        )\n",
    "      )\n",
    "      GROUP BY Date, DayGroup\n",
    "    )\n",
    "    WHERE DayGroupCount > {timeTh}\n",
    "    GROUP BY Date\n",
    "    \"\"\".format(windTh=windThreshold, timeTh=timeThreshold, fiveMin=FIVE_MINUTES_IN_SECS)\n",
    "    return spark.sql(queryStr)\n",
    "\n",
    "windyDaysGtEq15DF = windyDaysQuery(15,TWO_HOURS_IN_5MINS)\n",
    "windyDaysGtEq20DF = windyDaysQuery(20,TWO_HOURS_IN_5MINS)\n",
    "windyDaysGtEq25DF = windyDaysQuery(25,TWO_HOURS_IN_5MINS)\n",
    "\n",
    "#windyDaysGtEq15DF.show(1000)\n",
    "\n",
    "#WHERE DayGroupCount > 24\n",
    "#spark.sql(f\"\"\"\n",
    "#SELECT *, ROW_NUMBER() OVER (PARTITION BY )\n",
    "#FROM dayligthWindTable\n",
    "#\"\"\").show()\n",
    "\n",
    "#thresholdTrans(dayligthWindDF,15,TWO_HOURS_IN_5MINS).show()\n",
    "\n",
    "#dayListDF = dayligthWindDF.select(\"Date\").distinct().orderBy(\"Date\")\n",
    "#dayListDF.show()\n",
    "\n",
    "target = \"\"\"\n",
    "+----------+-------+-------+-------+------------------+\n",
    "|      Date|Total15|Total20|Total25|pseudoWindSpeedMPH|\n",
    "+----------+-------+-------+-------+------------------+\n",
    "|2012-01-01|     75|      0|      0|                15|\n",
    "|2012-01-02|     96|     25|      0|                20|\n",
    "|2012-01-03|    130|     60|     25|                25|\n",
    "|2012-01-04|      0|      0|      0|                 0|\n",
    "|2012-01-05|      0|      0|      0|                 0|\n",
    "|2012-01-07|      0|      0|      0|                 0|\n",
    "|2012-01-08|     46|      0|      0|                15|\n",
    "|2012-01-09|      0|      0|      0|                 0|\n",
    "|2012-01-10|      0|      0|      0|                 0|\n",
    "|2012-01-11|      0|      0|      0|                 0|\n",
    "|2012-01-12|      0|      0|      0|                 0|\n",
    "|2012-01-13|     58|      0|      0|                15|\n",
    "|2012-01-14|    131|     67|      0|                20|\n",
    "|2012-01-15|    132|     89|      0|                20|\n",
    "|2012-01-16|     92|      0|      0|                15|\n",
    "|2012-01-17|     61|      0|      0|                15|\n",
    "|2012-01-18|      0|      0|      0|                 0|\n",
    "|2012-01-19|     32|      0|      0|                15|\n",
    "|2012-01-20|     33|      0|      0|                15|\n",
    "|2012-01-21|      0|      0|      0|                 0|\n",
    "+----------+-------+-------+-------+------------------+\n",
    "\"\"\"\n",
    "\n",
    "#windyDaysDF.show()\n",
    "#windyDaysQuery(15,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the Pseudo-Speed of the Day\n",
    "\n",
    "As next step we join all the previous queries in a single table and we insert one more column called pseudoWindSpeedMPH. This captures the highest sustained speed above the predefined thresholds. If for instance there is a period of time that the wind blows for more than 2 hours at 25 mph then the pseudoWindSpeedMPH will be 25, it this is not the case then we look at the next range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholdTrans(df,windThreshold, timeThreshold) :\n",
    "    return (df.where(col(\"WindSpeedMPH\")>=windThreshold)\n",
    "        .withColumn(\"RowNum\",row_number().over(Window.partitionBy(\"Date\").orderBy(\"TSEpochSec\")))\n",
    "        .withColumn(\"DayGroup\",col(\"TSEpochSec\")-col(\"RowNum\")*lit(FIVE_MINUTES_IN_SECS))\n",
    "        .groupBy(\"Date\",\"DayGroup\").agg(count(lit(1)).alias(\"Count\"))\n",
    "        .where(col(\"Count\")>=timeThreshold)\n",
    "        .groupBy(\"Date\").agg(sum(col(\"Count\")).alias(\"Total{}\".format(windThreshold)))\n",
    "           )\n",
    "\n",
    "#windyDaysGtEq15DF_nosql = thresholdTrans(dayligthWindDF,15,TWO_HOURS_IN_5MINS)\n",
    "#windyDaysGtEq20DF_nosql = thresholdTrans(dayligthWindDF,20,TWO_HOURS_IN_5MINS)\n",
    "#windyDaysGtEq25DF_nosql = thresholdTrans(dayligthWindDF,25,TWO_HOURS_IN_5MINS)\n",
    "\n",
    "windyDaysDF = dayligthWindDF.select(\"Date\").distinct().orderBy(\"Date\") \\\n",
    "    .join(windyDaysGtEq15DF,[\"Date\"],\"left_outer\") \\\n",
    "    .join(windyDaysGtEq20DF,[\"Date\"],\"left_outer\") \\\n",
    "    .join(windyDaysGtEq25DF,[\"Date\"],\"left_outer\") \\\n",
    "    .fillna(0) \\\n",
    "    .withColumn(\"pseudoWindSpeedMPH\",\n",
    "                when(col(\"Total25\")>0,25)\n",
    "                .when(col(\"Total20\")>0,20)\n",
    "                .when(col(\"Total15\")>0,15)\n",
    "                .otherwise(0) )\n",
    "\n",
    "old = \"\"\"\n",
    "windyDaysDF = dayligthWindDF.select(\"Date\").distinct().orderBy(\"Date\") \\\n",
    "    .join(thresholdTrans(dayligthWindDF,15,TWO_HOURS_IN_5MINS),[\"Date\"],\"left_outer\") \\\n",
    "    .join(thresholdTrans(dayligthWindDF,20,TWO_HOURS_IN_5MINS),[\"Date\"],\"left_outer\") \\\n",
    "    .join(thresholdTrans(dayligthWindDF,25,TWO_HOURS_IN_5MINS),[\"Date\"],\"left_outer\") \\\n",
    "    .fillna(0) \\\n",
    "    .withColumn(\"pseudoWindSpeedMPH\",\n",
    "                when(col(\"Total25\")>0,25)\n",
    "                .when(col(\"Total20\")>0,20)\n",
    "                .when(col(\"Total15\")>0,15)\n",
    "                .otherwise(0) )\n",
    "\"\"\"\n",
    "#windyDaysDF.show()\n",
    "\n",
    "windyDaysDF.createOrReplaceTempView(\"windyDaysTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "windyDaysDF.show(100)\n",
    "windyDaysDF.coalesce(1).write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/windyDaysDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montly Report of Windy Days\n",
    "\n",
    "A monthly report can be extracted from the daily report. The information extracted is how many windy days we can expect as a percentage in a given month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the Proportion of Days\n",
    "\n",
    "For each wind range the days are grouped by month and a proportion oh how many days had wind in that range vs the total is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barChartDF = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  Month,\n",
    "  SUM(gteq15)/COUNT(1) AS Ratio15,\n",
    "  SUM(gteq20)/COUNT(1) AS Ratio20,\n",
    "  SUM(gteq25)/COUNT(1) AS Ratio25\n",
    "FROM (\n",
    "  SELECT *,\n",
    "    MONTH(Date) as Month,\n",
    "    CASE WHEN Total15 > 0 THEN 1 ELSE 0 END AS gteq15,\n",
    "    CASE WHEN Total20 > 0 THEN 1 ELSE 0 END AS gteq20,\n",
    "    CASE WHEN Total25 > 0 THEN 1 ELSE 0 END AS gteq25\n",
    "  FROM windyDaysTable\n",
    ")\n",
    "GROUP BY Month\n",
    "ORDER BY Month\n",
    "\"\"\")\n",
    "\n",
    "barChartPD = barChartDF.toPandas()\n",
    "barChartPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Chart Function\n",
    "\n",
    "There will be 3 bar for each month they will be overlapped so it is easier to see the proportions of the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMonthlySummary(barChartPD,ranges) :\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    for i, (name,label,color,height) in enumerate(ranges) :\n",
    "        ax.barh(\n",
    "            barChartPD['Month'],\n",
    "            barChartPD[name],\n",
    "            height=height,\n",
    "            align='center',\n",
    "            color=color,\n",
    "            edgecolor='Black',\n",
    "            label=label,\n",
    "            linewidth=0)\n",
    "\n",
    "    months = list(map(lambda x : calendar.month_name[x],barChartPD['Month']))\n",
    "    xticks = np.arange(0,100,10)/100\n",
    "    ax.set_yticks(barChartPD['Month'], minor=False)\n",
    "    ax.set_yticklabels(months, fontdict=None, minor=False)\n",
    "    ax.set_xticks(xticks, minor=False)\n",
    "    ax.set_xticklabels(map(lambda x : \"{:.0%}\".format(x),xticks), fontdict=None, minor=False)\n",
    "    #ax.set_xticklabels(map(lambda x : \"{:.0%}\".format(x),ax.get_xticks()), fontdict=None, minor=False)\n",
    "    ax.grid(linestyle='--',axis='x')\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    return (fig, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Chart Plot\n",
    "\n",
    "The function receives an array of tuples that has the columns that we want to plot from the dataframe, the label that will be used for the column in the plot, the color that we want to use and the width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangesSpec = [\n",
    "    ('Ratio15','>=15Mph','Orange',0.9),\n",
    "    ('Ratio20','>=20Mph','Red',0.7),\n",
    "    ('Ratio25','>=25Mph','Maroon',0.5)\n",
    "]\n",
    "\n",
    "fig, ax = plotMonthlySummary(barChartPD,rangesSpec)\n",
    "if DATABRICKS_ENV : display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar Report\n",
    "\n",
    "The final analysis of this work is to plot the value of the pseudoWindSpeedMPH in a calendar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Data\n",
    "\n",
    "The query to be performed is straightforward it is just extracting the Date and the pseudoWindSpeedMPH to a pandas dataframe. However, the dataframe has to be processed to create a timeseries for the plotting library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windyDaysPD = spark.sql(\"\"\"\n",
    "SELECT Date,pseudoWindSpeedMPH\n",
    "FROM windyDaysTable\n",
    "\"\"\").toPandas()\n",
    "windyDaysEventsPD = windyDaysPD.set_index(pd.DatetimeIndex(windyDaysPD['Date']))['pseudoWindSpeedMPH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calendar Heatmaps\n",
    "\n",
    "The library calmap is used to visualize the windy days. The darker the dot the higher the value of pseudoWindSpeedMPH. Calmap will show in gray the days with no reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = calmap.calendarplot(windyDaysEventsPD,linecolor='white', how=None ,fig_kws={'figsize':(10, 10)})\n",
    "if DATABRICKS_ENV : display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the Calendar Heatmaps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
